---
title: "Nyack Floodplain Data Archiving"
author: "Amalia Handler"
date: "6/24/2019"
output: 
  html_document: 
    keep_md: yes
editor_options: 
  chunk_output_type: console
---

Code to compile and organize Nyack Floodplain data meterological and hydrologic data into an archivable format.

Approach to start
- Figure out the structure of each df for HA07. Determine what is duplicate data and if anything has changed

Load the data
```{r include = FALSE}
options(stringsAsFactors = FALSE)
library(tidyverse)

# Get the data
temp <- read.delim("./Data/CR1000_HA15_Table1.dat", 
                   sep = ",", skip = 4, header = FALSE)

# Add the column headers
colnames(temp) <- colnames(read.delim("./Data/CR1000_HA15_Table1.dat", 
                                      sep = ",", skip = 1, header = TRUE))

head(temp)

ha07_files <- list.files(path = "./Data/HA07")

ha07_water_files <- ha07_files[grepl("_Water", ha07_files)]

# Testing te function
file_name <- "CR1000_HA07_Water - thru6_7_2016.dat"

# Write a function to load in the water files
load_ha07_water_file <- function(file_name){
  # Read in the file
  temp <- read.delim(paste("./Data/HA07/", file_name, sep = ''), 
                     sep = ",", skip = 4, header = FALSE)
  
  colnames(temp) <- colnames(read.delim(paste("./Data/HA07/", file_name, sep = ''), 
                                        sep = ",", skip = 1, header = TRUE))
  
  # Find the min and max sample dates in the file
  # Convert the timestamp variable to posix time
  temp$TIMESTAMP <- as.POSIXct(temp$TIMESTAMP, format = "%Y-%m-%d %H:%M:%S") 
  
  # Find the max
  max_time <- format(max(na.omit(temp$TIMESTAMP)), "%Y_%m_%d")
  
  # Find the min
  min_time <- format(min(na.omit(temp$TIMESTAMP)), "%Y_%m_%d")
  
  # Create a new file name using the station number, min, and max datetime
  new_name <- paste("HA07", min_time, "to", max_time, sep = '_')
  
  # Write the file to the same location in rds format
  saveRDS(temp, paste("./Data/HA07/", new_name, sep = ''))

}

# Apply the read in function to the water files in the directory
for(file in ha07_water_files){
  load_ha07_water_file(file)
}


ha07_1 <- readRDS(paste("./Data/HA07/", "HA07_2012_05_15_to_2012_07_06", sep = ''))
ha07_2 <- readRDS(paste("./Data/HA07/", "HA07_2012_07_06_to_2012_11_24", sep = ''))
ha07_3 <- readRDS(paste("./Data/HA07/", "HA07_2012_07_06_to_2013_03_31", sep = ''))
ha07_4 <- readRDS(paste("./Data/HA07/", "HA07_2013_04_01_to_2016_06_07", sep = ''))
ha07_5 <- readRDS(paste("./Data/HA07/", "HA07_2013_04_01_to_2018_07_10", sep = ''))
ha07_6 <- readRDS(paste("./Data/HA07/", "HA07_2013_04_01_to_2018_07_16", sep = ''))
ha07_7 <- readRDS(paste("./Data/HA07/", "HA07_2013_04_01_to_2019_06_05", sep = ''))

nrow(HA07_1)
nrow(HA07_2)



# Identify the rows that have NA values in the timestamp column
temp %>%
       rowid_to_column() %>%
     filter(is.na(TIMESTAMP))

```


I need to figure out earliest date for each station and the variables in each dataframe.

```{r include = FALSE}
# List the files in the directory from Phil
ha07_files <- list.files(path = "./Data/HA07")

# Identify only those files related to water
ha07_water_files <- ha07_files[grepl("_Water", ha07_files)]

# Read in the files
toread <- paste("./Data/HA07/", ha07_water_files, sep = '')

ha07_water <- list()
ha07_water <- lapply(toread, read.delim, header = FALSE, sep = ',', skip = 4) 

# Get the column names
var_names <- list()
for(i in 1:length(ha07_water_files)){
  var_names[[i]] <- colnames(read.delim(paste("./Data/HA07/", ha07_water_files[i], sep = ''), sep = ",", skip = 1, header = TRUE))
}

# Add the column names to the list of files
ha07_water <- lapply(ha07_water, function(x) {
  colnames(x) <- var_names[[1]]
  x      
})

# Convert the timestamp to a datetime variable
ha07_water <- lapply(ha07_water, function(x){
 x$TIMESTAMP <- as.POSIXct(x$TIMESTAMP, format = "%Y-%m-%d %H:%M:%S")
 return(x)
})


# Find the max and min data in each file
max_min_date <- lapply(ha07_water, function(x){
  min_date <- format(as.Date(min(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  max_date <- format(as.Date(max(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  dates <- c(min_date, max_date)
  return(dates)
})

# Name the dataframes in the list
ha07_water_df_names <- sapply(max_min_date, function(x){
                              paste("ha07_water", x[1], "to", x[2], sep = '_')
})

names(ha07_water) <- ha07_water_df_names

# Earliest date of water data in these files
min_date <- NULL
max_date <- NULL
for(i in 1:length(max_min_date)){
  min_date[i] <- max_min_date[[i]][1]
  max_date[i] <- max_min_date[[i]][2]
}

min_date <- as.Date(min_date, format = "%Y_%m_%d")
max_date <- as.Date(max_date, format = "%Y_%m_%d")

min(min_date)
max(max_date)


```

Now read in the meterological data from HA07

```{r include = FALSE}
# List the files in the directory from Phil
ha07_files <- list.files(path = "./Data/HA07")

# Identify only those files related to water
ha07_met_files <- ha07_files[grepl("_Met", ha07_files)]

# Read in the files
toread <- paste("./Data/HA07/", ha07_met_files, sep = '')

ha07_met <- list()
ha07_met <- lapply(toread, read.delim, header = FALSE, sep = ',', skip = 4) 

# Get the column names
var_names <- list()
for(i in 1:length(ha07_met_files)){
  var_names[[i]] <- colnames(read.delim(paste("./Data/HA07/", ha07_met_files[i], sep = ''), sep = ",", skip = 1, header = TRUE))
}

# Apply the column names to the list of dfs
ha07_met <- lapply(ha07_met, function(x) {
  colnames(x) <- var_names[[1]]
  x      
})

# Convert the timestamp to a datetime variable
ha07_met <- lapply(ha07_met, function(x){
 x$TIMESTAMP <- as.POSIXct(x$TIMESTAMP, format = "%Y-%m-%d %H:%M:%S")
 return(x)
})


# Find the max and min data in each file
max_min_date <- lapply(ha07_met, function(x){
  min_date <- format(as.Date(min(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  max_date <- format(as.Date(max(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  dates <- c(min_date, max_date)
  return(dates)
})

# Name the dataframes in the list
ha07_met_df_names <- sapply(max_min_date, function(x){
                              paste("ha07_met", x[1], "to", x[2], sep = '_')
})

names(ha07_met) <- ha07_met_df_names

# Earliest date of water data in these files
min_date <- NULL
max_date <- NULL
for(i in 1:length(max_min_date)){
  min_date[i] <- max_min_date[[i]][1]
  max_date[i] <- max_min_date[[i]][2]
}

min_date <- as.Date(min_date, format = "%Y_%m_%d")
max_date <- as.Date(max_date, format = "%Y_%m_%d")

min(min_date)
max(max_date)

```


Now load in the data from Jeremy

```{r include = FALSE}
# List the files in the directory from Phil
ha_files_toread <- list.files(path = "./Data", pattern = ".dat")

# Read in the files
toread <- paste("./Data/", ha_files_toread, sep = '')

ha_files <- list()
ha_files <- lapply(toread, read.delim, header = FALSE, sep = ',', skip = 4) 

# Get the column names
var_names <- list()
for(i in 1:length(ha_files)){
  var_names[[i]] <- colnames(read.delim(paste("./Data/", ha_files_toread[i], sep = ''), sep = ",", skip = 1, header = TRUE))
}

# Apply the column names to the list of dfs
for(i in 1:length(var_names)){
  colnames(ha_files[[i]]) <- var_names[[i]]
}

# Convert the timestamp to a datetime variable
ha_files <- lapply(ha_files, function(x){
 x$TIMESTAMP <- as.POSIXct(x$TIMESTAMP, format = "%Y-%m-%d %H:%M:%S")
 return(x)
})

# Find the max and min data in each file and determine if it is a water data files or a meteorlogical file
file_info <- lapply(ha_files, function(x){
  if(colnames(x)[3] == "AirTC") {
    data_type <- "met"
  } else { data_type <- "water"
  }
  min_date <- format(as.Date(min(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  max_date <- format(as.Date(max(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  dates <- c(data_type, min_date, max_date)
  return(dates)
})

# Find the ha number for each file
# If the file contains "CR1000", then I can extract
# If the file name contains "Nyack" then need another method
well_info <- NULL
for(i in 1:length(ha_files_toread)){
  if(grepl("CR1000", ha_files_toread[i])){
  well_info[i] <- substr(ha_files_toread[i], 8, 11)
  } else {
    well_info[i] <- substr(ha_files_toread[i], 7, 10)
  }
}

# Name the dataframes in the list
ha_df_names <- sapply(max_min_date, function(x){
                              paste("ha07_met", x[1], "to", x[2], sep = '_')
  })

ha_df_names <- NULL
for(i in 1:length(well_info)){
  ha_df_names[i] <- paste(well_info[i], file_info[[i]][1], file_info[[i]][2], "to", file_info[[i]][3], sep = "_")
}

names(ha_files)  <- ha_df_names
names(var_names) <- ha_df_names

# Need to coerce the mutiple columns in the HA02 df from character to number. Seems that R does not see "NAN" is equivelent to "NaN".
column_numbers <- c(11, 14:17)
for(col in column_numbers){
  ha_files$HA02_water_2013_04_01_to_2019_06_23[,col] <- 
    as.numeric(ha_files$HA02_water_2013_04_01_to_2019_06_23[,col])
}

```


Jeremy sent a whole set of data that include data prior to 2013-04-01
Going through the metadata for each station, I found the start date for each station. This is the date at which it was confirmed that the station is collecting reliable data.

Station, start date
CASC, 2012-09-06
HA02, 2012-06-08
HA07, 2012-05-15 (met station and well)
HA08, 2011-12-14 (well)
HA08, 2012-05-02 (Beaver Creek)
HA10, 2011-12-16
HA12, 2012-04-26
HA15, 2012-04-26

# Append this older data to the longer contemporary datasets that Jeremy sent

```{r}

# Find all the initial data files with data between the start of monitoring and the start of the data from Jeremy (2013-04-01)
ini_files <- c(
  './Data/Nyack_RiverNET_HA02 Movie Rd/Nyack HA02 MovRd_Table1 5_8_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA07 MET/CR1000_HA07_Met_5_15_12 thru 7_6_12.dat',
  './Data/Nyack_RiverNET_HA07 MET/CR1000_HA07_Met 7_6_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA07 MET/CR1000_HA07_Water_5_15_12 thru 7_6_12.dat',
  './Data/Nyack_RiverNET_HA07 MET/CR1000_HA07_Water 7_6_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA08_Cabin/Manual download/Nyack HA08 Cabin_Table1 5_2_12 well only.dat',
  './Data/Nyack_RiverNET_HA08_Cabin/Manual download/Nyack HA08 Cabin_Table1 5_2_12 thru 11_28_12.dat',
  './Data/Nyack_RiverNET_HA08_Cabin/CR1000_HA08 Cabin_Table1 8_23_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA10_Sarg/CR1000_HA10_Table1 12_16_11 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA12_Methane/CR1000_HA12_Table1 4_26_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA15_Springhead/CR1000_HA15_Table1 4_26_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_Cascadilla/Nyack CASC_Table1 9_6_12 thru 3_31_13.dat')

ini_ha_files <- lapply(ini_files, read.delim, header = FALSE, sep = ',', skip = 4) 

# Get the column names
var_names <- list()
for(i in 1:length(ini_ha_files)){
  var_names[[i]] <- colnames(read.delim(ini_files[i], sep = ",", skip = 1, header = TRUE))
}

# Apply the column names to the list of dfs
for(i in 1:length(var_names)){
  colnames(ini_ha_files[[i]]) <- var_names[[i]]
}

# Convert the timestamp to a datetime variable
ini_ha_files <- lapply(ini_ha_files, function(x){
 x$TIMESTAMP <- as.POSIXct(x$TIMESTAMP, format = "%Y-%m-%d %H:%M:%S")
 return(x)
})

# Apply names to the dataframes
names(ini_ha_files) <- c('ha02', 'ha07met', 'ha07met', 'ha07wtr', 'ha07wtr', 'ha08', 'ha08', 'ha08', 'ha10', 'ha12', 'ha15', 'casc')

# Coerce one of the column from one of the HA08 files from character to numeric
ini_ha_files[[6]]$DoConc2 <- as.numeric(ini_ha_files[[6]]$DoConc2)

# Bind together rows from the same station
ini_ha07met <- rbind(ini_ha_files[[2]], ini_ha_files[[3]])
ini_ha07wtr <- rbind(ini_ha_files[[4]], ini_ha_files[[5]])
ini_ha08    <- rbind(ini_ha_files[[6]], ini_ha_files[[7]], ini_ha_files[[8]])

# Remove duplicates from the above bound files
ini_ha07met <- ini_ha07met[!duplicated(ini_ha07met),]
ini_ha07wrt <- ini_ha07wtr[!duplicated(ini_ha07wtr),]
ini_ha08    <- ini_ha08[!duplicated(ini_ha08),]

# Make a new list, now with the bound, duplicates removed data
ini_ha_files <- list(ini_ha_files$casc, ini_ha_files$ha02, ini_ha07met, ini_ha07wtr, ini_ha08, ini_ha_files$ha10, ini_ha_files$ha12, ini_ha_files$ha15)

# Bind the initial data to the data from Jeremy

# First order the data
ha_files <- ha_files[order(names(ha_files))]

# Now bind together
ha_comb <- list()
for(i in 1:length(ha_files)){
  ha_comb[[i]] <- rbind(ini_ha_files[[i]], ha_files[[i]])
}

# Name the dfs
names(ha_comb) <- c('casc', 'ha02', 'ha07met', 'ha07wtr', 'ha08', 'ha10', 'ha12', 'ha15')

# Find the max and min data in each file and determine if it is a water data files or a meteorlogical file
file_info <- lapply(ha_comb, function(x){
  if(colnames(x)[3] == "AirTC") {
    data_type <- "met"
  } else { data_type <- "water"
  }
  min_date <- format(as.Date(min(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  max_date <- format(as.Date(max(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  dates <- c(data_type, min_date, max_date)
  return(dates)
})

# Renames the timestamp column and remove the records column in the met data
# Add a site column
ha07_met <- ha_comb$ha07met
ha07_met$site <- "ha07" 
ha07_met$sensor_number <- "met"
colnames(ha07_met)[1] <- "datetime"
ha07_met <- ha07_met[, c(21,22,1,3:20)]

# Save the meterological data file
# saveRDS(ha07_met, './Data/HA_Met_Data.rds')
# write.csv(ha07_met, './Data/HA_Met_Data.csv', row.names = FALSE)

```


Compile all water data into one df
Then separate into three spreadsheets for DO, Conductivity, and Water Level


```{r}

# For HA02, 08, and 15, there are two sets of sensors. Need to separate these into different dataframes
# HA02 has sensors 9 and 10
# HA08 has sensors 1 and 2
# HA15 has sensors 7 and 8
ha02_09 <- ha_comb$ha02[,colnames(ha_comb$ha02)[1:10]]
ha02_10 <- ha_comb$ha02[,colnames(ha_comb$ha02)[c(1,2,11:18)]]
ha08_01 <- ha_comb$ha08[,colnames(ha_comb$ha08)[1:10]]
ha08_02 <- ha_comb$ha08[,colnames(ha_comb$ha08)[c(1,2,11:18)]]
ha15_07 <- ha_comb$ha15[,colnames(ha_comb$ha15)[1:10]]
ha15_08 <- ha_comb$ha15[,colnames(ha_comb$ha15)[c(1,2,11:18)]]

# HA08_02 didn't come online until 2012-05-02. Need to trim this df to only include dates greater than and including this date. From looking at the dataframe, it looks like the first numbers appear at 11:00:00.

# TO avoid headaches with the rows that are missing datetime information, and because the df is organized in chronological order, I'll use a match function rather than a logical (>) to select the observations that come after 2012-05-02.
first_row <- match(as.POSIXct('2012-05-02 11:00:00'), ha08_02$TIMESTAMP)
ha08_02   <- ha08_02[first_row:nrow(ha08_02),] 

# Check the columns that have NA values for time
ha08_02[is.na(ha08_02$TIMESTAMP),]

# HA02_10 was initially installed in a parafluvial area, but quickly went dry and was removed from the area. It was removed on 2012-09-21 around 15:00. Note that the sensor was dry for some time before it was removed (first noted as dry on 2012-07-12)
# Looking at the data, the water level switches from positive to negative at 2012-07-11 11:00:00
# Remove data for this site after this point in time
last_row <- match(as.POSIXct('2012-07-11 11:00:00'), ha02_10$TIMESTAMP)
ha02_10  <- ha02_10[1:last_row,]

# Recombine into a list
ha_sep <- list(ha_comb$casc, ha02_09, ha02_10, ha_comb$ha07wtr,
               ha08_01, ha08_02, ha_comb$ha10, ha_comb$ha12,
               ha15_07, ha15_08)

# Set the site and sensor numbers
site <- c('CASC', 'HA02', 'HA02', 'HA07', 'HA08', 'HA08', 'HA10', 'HA12', 'HA15', 'HA15')

sensor <- sapply(ha_sep, function(x){
  col_name <- colnames(x)[10]
  if(nchar(col_name) == 8){
    num <- substr(col_name, 8, 8)
    sensor_number <- paste('0', num, sep = '')
  } else {
    sensor_number <- substr(col_name, 8, 9)
  }
  return(sensor_number)
})


# Add columns to each df for the site and the sensor number
for(i in 1:length(ha_sep)){
  ha_sep[[i]]$site <- site[i]
  ha_sep[[i]]$sensor_number <- sensor[i]
}

# Now bind together all the files
# Need to rename all the columns first for compatabiility
# New column names
col_names <- c('datetime', 'record', 'do_conc', 'do_sat', 'do_temp', 'cond', 'ct', 'cond_temp', 'level_m', 'level_temp', 'site', 'sensor_number')

# Rename the columns for the dfs in the list
ha_rename <- lapply(ha_sep, function(x){
  colnames(x) <- col_names
  return(x)
})

# Bind together all rows
ha_all <- do.call("rbind", ha_rename)

# Check that the number of rows in ha_all is the sum of the rows from ha_rename
sum(sapply(ha_rename, nrow)) == nrow(ha_all)
# Perfecto!

# Rearrange the columns
ha_water <- ha_all[, c(11,12,1,3:10)]

# Save the new master file
# saveRDS(ha_water, './Data/All_HA_Water_Data.rds')
# write.csv(ha_water, './Data/All_HA_Water_Data.csv', col.names = TRUE, row.names = FALSE)

```

What kind of NA values are in the df?

```{r}
# ha_water <- readRDS('./Data/All_HA_Water_Data')

# What timea are missing
temp <- subset(ha_water, is.na(ha_water$datetime))

# Check for missing datetime information
# Datetime was always missing around March 10, always at 02:00:00, must be due to daylight savings time. In spring, clocks go from 2 to 3 am instantly, thus losing an hour. Yet, there is still an observation collected between 1 and 3 am on these dates. Why?
ha_water[(which(ha_water$site == "CASC" & is.na(ha_water$datetime)) - 1),]
ha_water[(which(ha_water$site == "CASC" & is.na(ha_water$datetime)) + 1),]

ha_water[(which(ha_water$site == "HA02" & is.na(ha_water$datetime)) - 1),]
ha_water[(which(ha_water$site == "HA02" & is.na(ha_water$datetime)) + 1),]

ha_water[(which(ha_water$site == "HA07" & is.na(ha_water$datetime)) - 1),]
ha_water[(which(ha_water$site == "HA07" & is.na(ha_water$datetime)) + 1),]

ha_water[(which(ha_water$site == "HA08" & is.na(ha_water$datetime)) - 1),]
ha_water[(which(ha_water$site == "HA08" & is.na(ha_water$datetime)) + 1),]

ha_water[(which(ha_water$site == "HA10" & is.na(ha_water$datetime)) - 1),]
ha_water[(which(ha_water$site == "HA10" & is.na(ha_water$datetime)) + 1),]

ha_water[(which(ha_water$site == "HA12" & is.na(ha_water$datetime)) - 1),]
ha_water[(which(ha_water$site == "HA12" & is.na(ha_water$datetime)) + 1),]

ha_water[(which(ha_water$site == "HA15" & is.na(ha_water$datetime)) - 1),]
ha_water[(which(ha_water$site == "HA15" & is.na(ha_water$datetime)) + 1),]


# Identifying missing data based on when the sensors were serviced

# As an example, sensors were out at HA02_09 at 11:00 on 2012-06-26
time_out <- as.POSIXct('2012-06-26 11:00', format = '%Y-%m-%d %H:%M')
site     <- "HA02"
sensor   <- "09"

temp <- ha_water[ha_water$site == site & ha_water$sensor_number == sensor,]

row_num <- match(time_out, temp$datetime)

temp[(row_num - 3):(row_num + 3), 3:11]

# As an example, sensors were out at HA02_09 at 11:00 on 2012-06-26
time_out <- as.POSIXct('2012-07-23 19:00', format = '%Y-%m-%d %H:%M')
site     <- "HA02"
sensor   <- "09"

temp <- ha_water[ha_water$site == site & ha_water$sensor_number == sensor,]

row_num <- match(time_out, temp$datetime)

temp[(row_num - 3):(row_num + 3), 3:11]

# Another example
time_out <- as.POSIXct('2012-07-26 12:00', format = '%Y-%m-%d %H:%M')
site     <- "HA10"
sensor   <- "04"

temp <- ha_water[ha_water$site == site & ha_water$sensor_number == sensor,]

row_num <- match(time_out, temp$datetime)

temp[(row_num - 3):(row_num + 3), 3:11]

# Another example
time_out <- as.POSIXct('2014-10-07 14:00', format = '%Y-%m-%d %H:%M')
site     <- "HA15"
sensor   <- "07"

temp <- ha_water[ha_water$site == site & ha_water$sensor_number == sensor,]

row_num <- match(time_out, temp$datetime)

temp[(row_num - 3):(row_num + 3), 3:11]

# Example from a thing in surface water
time_out <- as.POSIXct('2014-05-08 11:00', format = '%Y-%m-%d %H:%M')
site     <- "CASC"
sensor   <- "06"

temp <- ha_water[ha_water$site == site & ha_water$sensor_number == sensor,]

row_num <- match(time_out, temp$datetime)

temp[(row_num - 3):(row_num + 3), 3:11]

# In general, when the sensors are being serviced, the water level is negative, the ct is negative, and the conductivity is below 0.005 mS (below 0.010)

# Let's see if I can pull out this data
bad_data <- ha_water[which(ha_water$cond < 0.010 & ha_water$ct < 0 & ha_water$level_m < 0),]
# Seems like this is flagging data from service trips where the sensors were pulled for a measurement.
# All sites now have approximately the same number of flagged observations
# From comparing the metadata to the datetimes flagged in this df, there are some observations that were flagged at times when there is no note in the metadata about removing sensors. This seems fairly reasonable. If someone was working in the well, but wasn't out with Tom or Phil, then it probably didn't make it into the metadata notes.
# Note also that this processes misses observations that satisfy one or two of these qualifiers, but have NA or NaN values for one or two of the others.
nrow(bad_data)

bad_do <- (ha_water[ha_water$do_conc <= 0 | is.na(ha_water$do_conc),])

# First thing that jumps out is that the ice up on CASC meant that the sensors could not be cleaned or calibrated and the DO readings for this period are all zero. Ice cover first recorded 11/27/2013, pouring ice melt down the well enabled calibration on 12/27/2013, then were frozen again until 4/3/2014

temp <- bad_do[which(bad_do$site == "CASC" & bad_do$do_conc == 0),]
unique(as.Date(temp$datetime))



```

Try out some different anomomly detection methods

```{r}
# Anomalize R Package

# devtools::install_github("business-science/anomalize")

library(tidyverse)
library(anomalize)
library(dplyr)
library(ggplot2)
library(tibbletime)

tidyverse_cran_downloads %>%
    ggplot(aes(date, count)) +
    geom_point(color = "#2c3e50", alpha = 0.25) +
    facet_wrap(~ package, scale = "free_y", ncol = 3) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
    labs(title = "Tidyverse Package Daily Download Counts",
         subtitle = "Data from CRAN by way of cranlogs package")

# Test out the anomoly detection for a small subset of data
# DO temperature from 01-2013 to 06-2013 for HA07
do_temp <- ha_water[ha_water$site == 'HA07' & ha_water$sensor_number == '03' & ha_water$datetime >= as.POSIXct('2013-01-01 00:00') & ha_water$datetime <= as.POSIXct('2013-06-01 00:00'), c('do_temp','datetime')]

do_temp <- as_tibble(do_temp)
do_temp <- do_temp[complete.cases(do_temp),]
do_temp <- as_tbl_time(do_temp, datetime)

do_temp %>%
    # Data Manipulation / Anomaly Detection
    time_decompose(do_temp, method = "stl") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose() %>%
    # Anomaly Visualization
    plot_anomalies(time_recomposed = TRUE, ncol = 3, alpha_dots = 0.25) +
    labs(title = "HA07 DO Temp Anomalies", subtitle = "STL + IQR Methods") 

# Getting an error where the function seems to not recognize the time component to the tibbletime object

# Try to reporduce the error
# Make a tibble with similar data, a smooth line with anomolous datapoints
# Create some test data
count <- c(1:25, 40, 27:50, 25, 52:75, 105, 77:100)
datetime_r <- seq(from = as.POSIXct('2013-01-01 00:00'), by = "1 hour", length.out = 100) 

# Insert an NA value into the datetime variable
datetime_r[25] <- NA

# Create a tibble object
test <- tibble(count, datetime_r)

# Run the amonalize function
test %>%
    # Data Manipulation / Anomaly Detection
    time_decompose(count) %>%
    anomalize(remainder, alpha = 0.05, max_anoms = 0.2) %>%
    time_recompose() %>%
    # Anomaly Visualization
    plot_anomalies(time_recomposed = TRUE) +
    labs(title = "Anomalies for Test Data") 





tbltime_test %>%
  time_decompose(y) %>%
  anomalize(remainder, alpha = 0.05, max_anoms = 0.2) %>%
  time_recompose() %>%
  plot_anomalies(time_recomposed = TRUE) +
  ggtitle("Anomalies for Test Data")

# Get only lubridate downloads
lubridate_dloads <- tidyverse_cran_downloads %>%
    filter(package == "lubridate") %>% 
    ungroup()

tidyverse_cran_downloads %>%
    # Data Manipulation / Anomaly Detection
    time_decompose(count, method = "stl") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose() %>%
    # Anomaly Visualization
    plot_anomalies(time_recomposed = TRUE, ncol = 3, alpha_dots = 0.25) +
    labs(title = "Tidyverse Anomalies", subtitle = "STL + IQR Methods") 

lubridate_dloads %>%
    # Twitter + GESD
    time_decompose(count, method = "stl") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose() %>%
    # Anomaly Visualziation
    plot_anomalies(time_recomposed = TRUE) +
    labs(title = "Lubridate Anomalies", subtitle = "STL + IQR Methods")




```



Visualize the 2013-present data

Explore the continuity in the data by plotting the whole record

Dissolved oxygen data

```{r}

# If I want to use the markdown capabilities
# echo = FALSE, fig.width = 4, fig.height = 3, res = 300, fig.cap = "DO concentration by well."

# To compile the water data from each site, exclude the meterological data
x <- ha_files[2:8]

png("./Figures/HA_DO_Conc.png", units = "in", 
    width = 12, height = 12, res = 300, bg = "transparent")

par(mfrow = c(5,2), oma = c(2,2,1,1), mar = c(3,5,2,0))

lapply(seq_along(x), function(i){
  if(ncol(x[[i]]) == 10) {
  plot(x[[i]][,1], x[[i]][,3], xlab = " ", ylab = "DO (mg/L)", 
       cex.lab = 2, cex.axis = 2)
  title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[3], sep = ' '), 
        line = 0.5, cex.main = 2)
  box(lwd = 2)
  } else {
       plot(x[[i]][,1], x[[i]][,3], xlab = " ", ylab = "DO (mg/L)", 
       cex.lab = 2, cex.axis = 2)
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[3], sep = ' '), 
             line = 0.5, cex.main = 2)
       box(lwd = 2)
      
       plot(x[[i]][,1], x[[i]][,11], xlab = " ", ylab = "DO (mg/L)", 
       cex.lab = 2, cex.axis = 2)
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[11], sep = ' '), 
       line = 0.5, cex.main = 2)
       box(lwd = 2)
  }
})

dev.off()
```


Conductivity Data

```{r}
# x <- ha_files[2:8]

y_label   <- "Cond (mS/cm)"
first_col <- 6
sec_col   <- 14

png("./Figures/HA_Cond.png", units = "in", 
    width = 12, height = 12, res = 300, bg = "transparent")

par(mfrow = c(5,2), oma = c(2,2,1,1), mar = c(3,5,2,0))

lapply(seq_along(x), function(i){
  if(ncol(x[[i]]) == 10) {
  plot(x[[i]][,1], x[[i]][,first_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2)
  title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[first_col], sep = ' '), 
        line = 0.5, cex.main = 2)
  box(lwd = 2)
  } else {
       plot(x[[i]][,1], x[[i]][,first_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2)
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[first_col], sep = ' '),
             line = 0.5, cex.main = 2)
       box(lwd = 2)
      
       plot(x[[i]][,1], x[[i]][,sec_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2)
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[sec_col], sep = ' '), 
       line = 0.5, cex.main = 2)
       box(lwd = 2)
  }
})

dev.off()

```


Water level

```{r}
# x <- ha_files[2:8]

y_label   <- "Water Level (m)"
first_col <- 9
sec_col   <- 17

png("./Figures/HA_WaterLevel.png", units = "in", 
    width = 12, height = 12, res = 300, bg = "transparent")

par(mfrow = c(5,2), oma = c(2,2,1,1), mar = c(3,5,2,0))

lapply(seq_along(x), function(i){
  if(ncol(x[[i]]) == 10) {
  plot(x[[i]][,1], x[[i]][,first_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2)
  title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[first_col], sep = ' '), 
        line = 0.5, cex.main = 2)
  box(lwd = 2)
  } else {
       plot(x[[i]][,1], x[[i]][,first_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2)
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[first_col], sep = ' '),
             line = 0.5, cex.main = 2)
       box(lwd = 2)
      
       plot(x[[i]][,1], x[[i]][,sec_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2)
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[sec_col], sep = ' '), 
       line = 0.5, cex.main = 2)
       box(lwd = 2)
  }
})

dev.off()

```


Temperature associated with the DO sensor

```{r}
# x <- ha_files[2:8]

y_label   <- "Temp (C)"
first_col <- 5
sec_col   <- 13

png("./Figures/HA_DO_Temp_ylim20.png", units = "in", 
    width = 12, height = 12, res = 300, bg = "transparent")

par(mfrow = c(5,2), oma = c(2,2,1,1), mar = c(3,5,2,0))

lapply(seq_along(x), function(i){
  if(ncol(x[[i]]) == 10) {
  plot(x[[i]][,1], x[[i]][,first_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2, ylim = c(0,20))
  title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[first_col], sep = ' '), 
        line = 0.5, cex.main = 2)
  box(lwd = 2)
  } else {
       plot(x[[i]][,1], x[[i]][,first_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2, ylim = c(0,20))
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[first_col], sep = ' '),
             line = 0.5, cex.main = 2)
       box(lwd = 2)
      
       plot(x[[i]][,1], x[[i]][,sec_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2, ylim = c(0,20))
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[sec_col], sep = ' '), 
       line = 0.5, cex.main = 2)
       box(lwd = 2)
  }
})

dev.off()

```

EPA Job Talk Figure

```{r}

vis <- ha_files$HA07_water_2013_04_01_to_2019_06_25

png("./Figures/EPA Job Talk Fig.png", units = "in", 
    width = 5, height = 8, res = 300, bg = "transparent")

par(mfrow = c(4,1), oma = c(2,2,1,1), mar = c(3,5,2,0), c(3, 2, 0))

plot(vis$TIMESTAMP, vis$Lvl_m3, xlab = " ", ylab = "Water Level", 
       cex.lab = 2, cex.axis = 2, ylim = c(3,4.5), pch = 20, xaxt='n')
  title("Water Level (m)", line = 0.5, cex.main = 2)
  box(lwd = 2)
  
plot(vis$TIMESTAMP, vis$DO_W_T3, xlab = " ", ylab = "Temp", 
       cex.lab = 2, cex.axis = 2, ylim = c(4,10), pch = 20, xaxt='n')
  title("Temperature (Â°C)", line = 0.5, cex.main = 2)
  box(lwd = 2)
  
plot(vis$TIMESTAMP, vis$Cond3, xlab = " ", ylab = "Cond", 
       cex.lab = 2, cex.axis = 2, ylim = c(.2,.3), pch = 20, xaxt='n')
  title("Conductivity (mS)", line = 0.5, cex.main = 2)
  box(lwd = 2)
  
plot(vis$TIMESTAMP, vis$DoConc3, xlab = " ", ylab = "DO", 
       cex.lab = 2, cex.axis = 2, ylim = c(4,8), pch = 20)
  title("Dissolved Oxygen (mg/L)", line = 0.5, cex.main = 2)
  box(lwd = 2)  

dev.off()


```

